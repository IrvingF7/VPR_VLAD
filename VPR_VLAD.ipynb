{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27d12f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import BallTree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48377df",
   "metadata": {},
   "source": [
    "# Extract and Aggregate SIFT Feature of the Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ade95f",
   "metadata": {},
   "source": [
    "1. **Extract** and **Aggregate** features from all the databse images. I use SIFT here, but SURF or ORB may also work. Every step, we directly read an image from the SSD, extract featrue with help of OpenCV 4.4+, aggregate the extracted feature into a big list `database_entire_des` that would store all the features from all the images, by taking advantage of python's `extend` so we are not creating nested array.\n",
    "    - It's tempting to store all the loaded images or computed (SIFT) feature and retrieve them later from RAM. But my experiment showed that it's actually faster if we just load them and re-compute (SIFT) feature whenever we need any one of these two, as SSD + CPU doing heavylifting is faster than CPU + RAM doing heavylifing. Anyway, I didn't store the computed SIFT or loaded images anywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "897b5d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_path = r'./T4imgs/database/'\n",
    "query_path = r\"./T4imgs/queries/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "5ee49ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIFT_extractor = cv2.SIFT_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "c103f3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_aggregate_feature(extractor, folder_path):\n",
    "    all_descriptors = list()\n",
    "    database_feature = dict()\n",
    "    \n",
    "    for img_name in tqdm(os.listdir(folder_path)):\n",
    "        \n",
    "        # get feature\n",
    "        img = cv2.imread(folder_path + img_name)\n",
    "        kp, des = extractor.detectAndCompute(img, None)\n",
    "        \n",
    "        # append the descriptor to the aggregated list\n",
    "        all_descriptors.extend(des)\n",
    "    \n",
    "    # use np.asarray so we don't copy and save some memory\n",
    "    all_descriptors = np.asarray(all_descriptors)\n",
    "\n",
    "    return all_descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "c57c29d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 28600/28600 [06:36<00:00, 72.09it/s]\n"
     ]
    }
   ],
   "source": [
    "database_entire_des = extract_aggregate_feature(SIFT_extractor, database_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71f0de1",
   "metadata": {},
   "source": [
    "# Clustering the Features using K-mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789aadd8",
   "metadata": {},
   "source": [
    "2. **Clustering** the entire list of features by using scikit-learn's KMeans class. \n",
    "    - We cluster them into 16 class and only run it 1 times, because in our case we are actually finding the identical image in the database given a query, and the image is very small itslef. In the real world if the query image is larger and not even in the database, so we can only find similar one, we would need more classes (64 or 256) and more run of the clustering algorithm to get the best performing one (like 10 times) so the generated codebook is more robust.\n",
    "    - I turned down the text display for training so it's less verbose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "d1fdee85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# clustering the entire bag of descriptor\n",
    "codebook = KMeans(n_clusters = 16, init='k-means++', n_init=1, verbose=0).fit(database_entire_des)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537db22e",
   "metadata": {},
   "source": [
    "# Compute VLAD Feature for Every Database Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbddb519",
   "metadata": {},
   "source": [
    "3. **Compute** VLAD feature for every image. We first load all images, compute their (SIFT) features on the fly as it's faster like I said before. We then use this feature to compute the corresponding VLAD feature of this image, and append it to the list `database_VLAD`, where each element is a VLAD feature.\n",
    "    - We compute VLAD by compute the sum of residue to each centroid and concatenate these vectors.\n",
    "    - We normalize the VLAD vector using square root normalization and L2 normalization.\n",
    "    - My implementation of VLAD calculation is adapted from here: https://github.com/jorjasso/VLAD/blob/eeaad96c33aea9c11bceb285ba5cdabba9fb663f/VLADlib/VLAD.py#L177\n",
    "4. At the same time we create a list `database_name` used to hold all the names of database image. Because we are inserting name to this list at the same time we create and append a VLAD feature, we now have a one-to-one mapping between `database_VLAD` and `database_name`, i.e. two list is pointing to the same image if given two identical index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "387fc4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_VLAD(X, codebook):\n",
    "\n",
    "    predictedLabels = codebook.predict(X)\n",
    "    centroids = codebook.cluster_centers_\n",
    "    labels = codebook.labels_\n",
    "    k = codebook.n_clusters\n",
    "   \n",
    "    m,d = X.shape\n",
    "    VLAD_feature = np.zeros([k,d])\n",
    "    #computing the differences\n",
    "\n",
    "    # for all the clusters (visual words)\n",
    "    for i in range(k):\n",
    "        # if there is at least one descriptor in that cluster\n",
    "        if np.sum(predictedLabels == i) > 0:\n",
    "            # add the diferences\n",
    "            VLAD_feature[i] = np.sum(X[predictedLabels==i,:] - centroids[i],axis=0)\n",
    "    \n",
    "\n",
    "    VLAD_feature = VLAD_feature.flatten()\n",
    "    # power normalization, also called square-rooting normalization\n",
    "    VLAD_feature = np.sign(VLAD_feature)*np.sqrt(np.abs(VLAD_feature))\n",
    "\n",
    "    # L2 normalization\n",
    "    VLAD_feature = VLAD_feature/np.linalg.norm(VLAD_feature)\n",
    "    return VLAD_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "bdad5d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 28600/28600 [07:12<00:00, 66.16it/s]\n"
     ]
    }
   ],
   "source": [
    "database_VLAD = list()\n",
    "database_name = list()\n",
    "for img_name in tqdm(os.listdir(database_path)):\n",
    "    img = cv2.imread(database_path + img_name)\n",
    "    kp, des = SIFT_extractor.detectAndCompute(img, None)\n",
    "    VLAD = get_VLAD(des, codebook)\n",
    "    database_VLAD.append(VLAD)\n",
    "    database_name.append(img_name)\n",
    "    \n",
    "database_VLAD = np.asarray(database_VLAD)\n",
    "    #database_VLAD[VLAD] = img_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a58650",
   "metadata": {},
   "source": [
    "# Indexing all the VLAD Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e271a07",
   "metadata": {},
   "source": [
    "5. **Indexing** all the VLAD features by creating a `BallTree` of the list `database_VLAD`. This is not essential because we can also do a pair-wise comparison, but BallTree saves a lot of time when retrieving the item that has the smallest distance to the query. This is not essential so I will skip explaining Balltree. But generally, it's a efficient indexing method that performs better when the data is high dimensional, comparing to its alternative KD-Tree\n",
    "    - I am using L2 distance as the measure between VLAD features. But the choice doesn't matter in this specific problem, because again, we are finding the exact same picture so the distance, no matter what, would be 0, since SIFT and VLAD are both not randomized, so for the same image they would generate the same feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "3a97cb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = BallTree(database_VLAD, leaf_size=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016ef4d2",
   "metadata": {},
   "source": [
    "# Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6cef29",
   "metadata": {},
   "source": [
    "6. Compute (SIFT) feature of all the query images, and then compute VLAD feature accordingly, using the same clustering as the database. We then get the VLAD feature of this 5 query images. We then find the images in database whose VLAD feature distance to these query images are 0, respectively.\n",
    "    - Here we are finding only the top 1 match in `tree.query`, because we know the distance will be 0 as the image in query will have an identical one in the database. In the real world when finding similar images, we will need more matching like 3 or 5, and manually or use some other algorithm to further identify the most similar one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "1a90e217",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_path = r\"./T4imgs/queries/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "0cde7afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 10.23it/s]\n"
     ]
    }
   ],
   "source": [
    "value_list = list()\n",
    "for img_name in tqdm(os.listdir(query_path)):\n",
    "    query = cv2.imread(query_path+img_name)\n",
    "    q_kp, q_des = SIFT_extractor.detectAndCompute(query, None)\n",
    "    query_VLAD = get_VLAD(q_des, codebook).reshape(1, -1)\n",
    "    \n",
    "    # we only want the cloest one\n",
    "    dist, index = tree.query(query_VLAD, 1)\n",
    "    \n",
    "    # index is an array of array of 1\n",
    "    value_name = database_name[index[0][0]]\n",
    "    value_list.append(value_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9dc62c",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044ea6e2",
   "metadata": {},
   "source": [
    "7. Below is the matches in the database for all the query images, in the order from query1.png to query5.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "b2a4b115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image1373.png',\n",
       " 'image2622.png',\n",
       " 'image6051.png',\n",
       " 'image26588.png',\n",
       " 'image13935.png']"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
